---
title: "miRcore R camp Random Forest Test"
author: "Madden Moore, updated by Inhan Lee"
date: "updated July 2023, original June 11, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = TRUE)
```

### (c)miRcore

## Required Packages

Run the following code chunk to load in all of the required packages for this code. If you do not have any of these packages installed, run `install.packages("packagename")` in the console, to knit html file.

```{r}
install.packages("randomForest")
library(randomForest)
```


## Preparing Data

### Reading in the data

As with other txt files, we want to read in our data using the `read.table` function, specifying that we have a header and our first column is our row names. Make sure you have the file in the current working directory.

```{r}
brca_miR_data = read.table("full_BRCA_miR_data.txt", header=T, row.names=1, stringsAsFactors=F)
```

### Selecting Significant miRNAs

For our Random Forest, we only want to use the statistically significant miRNAs (p-value < 0.05) for input. We will loop through every miRNA, perform a t-test using that miRNA, and add it to our significant miRNAs vector if it is significant (p-value <0.001).

```{r}
sig_miRs <- vector()
num_miRNAs <- length(brca_miR_data)
tumor_samples <- 1:86
control_samples <- 87:172
for(i in 1:num_miRNAs)
{
  tumor_vector <- brca_miR_data[i, tumor_samples]
  control_vector <- brca_miR_data[i, control_samples]
  p_value <- t.test(tumor_vector, control_vector)$p.value
  #print(p_value)
  #print(p_value < 0.05)
  if(!is.na(p_value) && p_value < 0.001)
  {
    sig_miRs <- c(sig_miRs, i)
  }
}
sig_miRs
```

Note how we first have to check that our p value is not NA. This is because comparing NA to 0.001 produces an error. Because of how R evaluates code, it won't even try the second comparison in the and statement if the first test turns out to be false (if p_value is NA). 

```{r}
length(sig_miRs)
```

We have 58 statistically significant miRNAs. Now we just need to select only these rows from our data.

```{r}
brca_miR_data = brca_miR_data[sig_miRs, ]
```

```{r}
dim(brca_miR_data)
```

We have successfuly shrunk our data down to 58 miRNAs. 

### Transposing

Because we want each input into the machine learning model to be an individual patient instead of an individual miRNA, we need to `transpose` the data (switch rows and columns) before continuing.

This can be done using the `t()` function.

```{r}
brca_miR_data = t(brca_miR_data)
```

To illustrate the effects of this command, we can view the first few rows and columns of the data.

```{r}
head(brca_miR_data)
```

### Adding Condition Column

For our final input into the machine learning model, we want to make the first column have information on whether the cell was a tumor cell or a control cell. 

First, we want to extract the rownames from our dataframe. We also want to clear the existing rownames. 

```{r}
Condition = rownames(brca_miR_data)
```

Next, we want to rename each patient to be either `Tumor` or `Control`, as we don't care which number patient they are anymore.
To do this, we will use grepl which return a true/false instead of grep which returns an index value.

```{r}
for(i in 1:length(Condition))
{
  if(grepl("Tumor", Condition[i], fixed = TRUE) == TRUE)
  {
    Condition[i] = "Tumor"
  }
  else {
    Condition[i] = "Control"
  }
}
```

What this does is checks if `Tumor` is in each condition. If it is, it renames that condition to just `Tumor`. Otherwise, it renames it to just `Control`. 

Now that we have the conditions, we want to add this as a column to our original dataset.

```{r}
Condition = as.factor(Condition)
brca_miR_data = cbind.data.frame(Condition, brca_miR_data)
brca_miR_data[1:10, 1:15]
```

## Preparing Train and Test

For this camp, we will use a 70/30 split between training and testing, meaning that 70% of the data is used to train the model, and the trained model is tested on the remaining 30%.

Since each row represents one patient, we can just take a random sample of numbers in the range of the number of rows.

```{r}
set.seed(123)
num_row = length(Condition)
train_sample = sample(1:num_row, round(0.7*num_row), replace = FALSE)
```

Let's break this down. The first argument for the sample command is the range we want to select our samples from. In our case, that is all integers from 1 to the total number of rows. The second argument is how many values we want to sample. Because num_rows * 0.7 isn't an integer, we want to round it to the nearest integer. Finally, we don't want to draw the same sample twice, so we set replace to FALSE.

Additionally, we want to set the random seed (the value that determines how computer randomness works, since computer randomness isn't really random) to a set number so we can reproduce these results. If you don't want consistent results every time you run this script, you can comment out that line.

Now that we have our samples, we can create our training matrix, which is just the rows from train_samples. To help the randomForest function later, we need to replace all dashes with periods.

```{r}
colnames(brca_miR_data) = gsub("-", ".", colnames(brca_miR_data))
train_data <- brca_miR_data[train_sample,]
```

For the testing data, we just want the rows that weren't included in the training data. The `-` operator can help us with this. We also need to modify the column names again.

```{r}
test_data = brca_miR_data[-train_sample, ]
```

What this does is it takes all the rows that are not in train_samples.

```{r}
dim(train_data)
dim(test_data)
```

We have 120 cases in training and 52 in testing. There are 59 columns because we have our 58 miRNAs + 1 colummn for the condition.

## Running the model

Now that we have our train and test datasets, we can begin the machine learning. We will use random forests for this script.

```{r}
brca_forest = randomForest(Condition ~ ., ntree = 100, data = train_data)
importance(brca_forest)[order(importance(brca_forest)),]
varImpPlot(brca_forest)
```
Higher Mean Decrease Gini scores indicates greater importance of variable



Let's see how our model performed on the training dataset:

```{r}
train_predictions = predict(brca_forest, train_data)
table(train_predictions, train_data$Condition)
```
The model was 100% accurate. This is to be expected, as it already knew the answers!

The real test of model performance is how it performs on external data it has never seen before. In this scenario, we want to use the testing data, which we separated from the training data earlier.

```{r}
test_predictions = predict(brca_forest, test_data)
table(test_predictions, test_data$Condition)
```

Only 1 error out of 52 cases - not bad!

